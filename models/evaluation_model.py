import os
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
)


class EvaluationModel:
    def __init__(self):
        '''Initializes evaluation model'''
    
        model_name = "models/google/gemma-2b-it"

        if not os.path.exists(model_name):
            raise Exception(
                f"""
            The evaluators expect the model weights to be checked into the repository,
            but we could not find the model weights at {model_name}

            Please follow the instructions in the docs/download_baseline_model_weights document 
            to download and check in the model weights.
            """
            )

        self.tokenizer = AutoTokenizer.from_pretrained(model_name)

        self.llm = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map="auto",
            torch_dtype=torch.float16
        )

    def respond(self, query):
        
        prompt = self.tokenizer.apply_chat_template(query, tokenize=False, add_generation_prompt=True)
        inputs = self.tokenizer.encode(prompt, add_special_tokens=False, return_tensors="pt")
        # Create attention mask tensor
        attention_mask = torch.ones_like(inputs)  # Create a tensor of all 1s with the same shape as inputs
        # Generate outputs with attention mask
        outputs = self.llm.generate(input_ids=inputs.to(self.llm.device), 
                                    attention_mask=attention_mask.to(self.llm.device),
                                    max_new_tokens=5)
        outputs = self.tokenizer.decode(outputs[0])
        # Get only the repsponse generated by llm
        outputs = outputs.replace(prompt, "").replace("<eos>", "").strip()

        return outputs
